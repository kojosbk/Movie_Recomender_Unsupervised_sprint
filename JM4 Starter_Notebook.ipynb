{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Top'></a>\n",
    "# UNSUPERVISED CHALLENGE\n",
    "\n",
    "© Explore Data Science Academy\n",
    "\n",
    "---\n",
    "\n",
    "# Team JM4 Unsupervised Predict Notebook\n",
    "\n",
    "---\n",
    "<img src=\"img/predict bg.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id=\"cont\"></a>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "## <a href=#one>1. Introduction</a>\n",
    "* <a href=#problemstament>1.1 Problem Statement</a>\n",
    "* <a href=#po>1.2 Project Objectives</a>\n",
    "* <a href=#dotrain_eda>1.3 Data Overview</a>\n",
    "* <a href=#StartingaCometexperiment>1.4 Starting a Comet experiment</a>\n",
    "\n",
    "## <a href=#two>2. Import Necessary Libraries</a>\n",
    "\n",
    "## <a href=#three>3. Loading Datasets</a>\n",
    "* <a href=#three1>3.1 Set Pandas to enable viewing of all columns</a>\n",
    "* <a href=#three2>3.2 Check the \"Shape\" of the data-sets</a>\n",
    "* <a href=#three21>3.3 Use the \".column\" function to view the columns in our data set</a>\n",
    "\n",
    "## <a href=#four>4. Data Preprocessing (Cleaning)</a>\n",
    "* <a href=#four1>4.1 Identifying Missing Values</a>\n",
    "* <a href=#four2>4.2 Preprocessing the Dataset</a>\n",
    "<!-- * <a href=#four3>4.3 Most Common Words</a> -->\n",
    "\n",
    "## <a href=#five>5. Exploratory Data Analysis (EDA)</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id=\"one\"></a>\n",
    "## 1. INTRODUCTION\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Project Overview ⚡ |\n",
    "| :--------------------------- |\n",
    "| We will address the problem statement and objectives, as well as the classification of data aspects and a brief discussion of the Movie Recomender System in this part.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "- Recommender systems are economically and socially essential in today's technologically advanced world for enabling people to make wise decisions about the information they consume on a daily basis. This is particularly true in the area of movie content suggestions, where clever algorithms may guide viewers toward excellent films among tens of thousands of possibilities.\n",
    "\n",
    "- We will build a recommendation system based on collaborative filtering or content that will be capable of effectively predicting how a user would rate a film they haven't yet seen based on their past preferences..\n",
    "\n",
    "- With users of the system being exposed to content they would like to watch or buy, the provision of an accurate and robust solution to this problem has enormous economic potential, creating income and platform loyalty.\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id=\"problemstament\"></a> \n",
    " \n",
    "### 1.1 Problem Statement\n",
    "\n",
    "Our team has been charged with developing a collaborative filtering or content-based recommendation system that can precisely anticipate how a user would score a movie they haven't yet seen based on their prior preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"po\"></a>\n",
    "\n",
    "### 1.2 Project Objectives\n",
    "\n",
    "* examine the data provided, find any probable flaws, and clean the current data set\n",
    "\n",
    "* ascertain whether further characteristics may be included to improve the data set\n",
    "\n",
    "* create a model that can forecast how a consumer would evaluate a film\n",
    "\n",
    "* assess the best machine learning model's accuracy\n",
    "\n",
    "* using a user's previous choices to forecast with accuracy how they would evaluate a movie they haven't yet seen\n",
    "\n",
    "* Describe the model's inner workings to a non-technical audience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"dodf\"></a>\n",
    "\n",
    "### 1.3 Data Overview\n",
    "\n",
    "Millions of users of the online MovieLens movie recommendation service provided their 5-star ratings for this dataset, which comprises of several million ratings. In order to enhance the functionality of explicitly-based recommender systems, business and academic researchers have long used the MovieLens dataset. \n",
    "\n",
    "We'll be utilizing an enhanced, resampled, and unique version of the Movies dataset for this Predict in order to conduct a fair evaluation.\n",
    "\n",
    "#### I. Supplied Files\n",
    "* genome_scores.csv: a rating indicating how well movies match certain tag-related qualities.\n",
    "\n",
    "* genome_tags.csv: user-assigned tags for scores connected to the genome\n",
    "\n",
    "* imdb_data.csv: Using the links.csv file, more movie metadata is extracted from IMDB.\n",
    "\n",
    "* links.csv: File supplying a mapping between a related IMDB ID and TMDB ID and a Movies ID.\n",
    "\n",
    "* tags.csv: For each movie in the collection, a user-assigned number.\n",
    "\n",
    "* test.csv: The test split of the dataset. Contains user and movie IDs with no rating data.\n",
    "\n",
    "* train.csv: The dataset's training segment. Contains user and movie IDs and the ratings that go with them.\n",
    "\n",
    "\n",
    ">#### Additional Information\n",
    "The below information is provided directly from the Movies dataset description files:\n",
    "\n",
    "### Ratings Data File Structure (train.csv)\n",
    "- The train.csv file contains all ratings. Following the header row, each line of this file contains one user's rating of one movie, in the following format:\n",
    "    - userId\n",
    "    - movieId\n",
    "    - rating\n",
    "    - timestamp\n",
    "\n",
    "- Within this file, the lines are arranged by userId first, then within user, by movieId.\n",
    "\n",
    "- A 5-star rating system is used, with half-star increments (0.5 stars - 5.0 stars).\n",
    "\n",
    "- Timestamps are seconds since midnight on January 1, 1970, Coordinated Universal Time (UTC).\n",
    "\n",
    "### Tags Data File Structure (tags.csv)\n",
    "- The file tags.csv contains all of the tags. After the header row, each line in this file indicates a tag a user has applied to a particular video, and it is formatted as follows:\n",
    "    - userId\n",
    "    - movieId\n",
    "    - tag\n",
    "    - timestamp\n",
    "\n",
    "- Within this file, the lines are arranged by userId first, then within user, by movieId.\n",
    "\n",
    "- User-generated metadata about movies is created using tags. Typically, each tag consists of a single word or brief sentence. Each user decides what a given tag means, is worth, and serves for.\n",
    "\n",
    "- Timestamps show seconds since midnight on January 1, 1970, Coordinated Universal Time (UTC).\n",
    "\n",
    "### Movies Data File Structure (movies.csv)\n",
    "- The movies.csv file contains movie-related data. Following the header row, each line in this file represents a single movie and is formatted as follows:\n",
    "    - movieId\n",
    "    - title\n",
    "    - genres\n",
    "\n",
    "- Genres are a pipe-separated list, and are selected from the following:\n",
    "    - Action\n",
    "    - Adventure\n",
    "    - Animation\n",
    "    - Children's\n",
    "    - Comedy\n",
    "    - Crime\n",
    "    - Documentary\n",
    "    - Drama\n",
    "    - Fantasy\n",
    "    - Film-Noir\n",
    "    - Horror\n",
    "    - Musical\n",
    "    - Mystery\n",
    "    - Romance\n",
    "    - Sci-Fi\n",
    "    - Thriller\n",
    "    - War\n",
    "    - Western\n",
    "    (no genres listed)\n",
    "\n",
    "### Links Data File Structure (links.csv)\n",
    "- The file links.csv contains identifiers that may be used to link to various sources of movie data. This file's lines after the header row represent individual movies and are formatted as follows:\n",
    "    - movieId\n",
    "    - imdbId\n",
    "    - tmdbId\n",
    "\n",
    "- movieId is the movie identification that https://movies.org uses. For instance, https://movielens.org/movies/1 provides a link to the movie Toy Story.\n",
    "\n",
    "- The movie identification used by http://www.imdb.com is called imdbId. Toy Story, for instance, may be found at http://www.imdb.com/title/tt0114709.\n",
    "\n",
    "- tmdb\n",
    "\n",
    "- Id is the movie identification that https://www.themoviedb.org uses. Toy Story, for instance, may be found at https://www.themoviedb.org/movie/862.\n",
    "\n",
    "### Tag Genome (genome-scores.csv and genome-tags.csv)\n",
    "- As previously mentioned, the tag genome encodes the degree to which movies display specific characteristics that are denoted by tags (atmospheric, thought-provoking, realistic, etc.). Using a machine learning algorithm on user-generated content, such as tags, ratings, and textual reviews, the tag genome was calculated.\n",
    "- Two files make up the genome. Movie-tag relevance information is presented in the following way in the file genome-scores.csv:\n",
    "    - movieId\n",
    "    - tagId\n",
    "    - relevance\n",
    "- The tag descriptions for the tag IDs in the genome file are provided in the second file, genome-tags.csv, in the following format:\n",
    "    - tagId\n",
    "    - tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"StartingaCometexperiment\"></a>\n",
    "\n",
    "### 1.4 Starting a Comet Experiment\n",
    "<img src=\"https://www.comet.ml/images/logo_comet_light.png\" width=\"350\" alt=\"Drawing\" style=\"width: 350px;\"/>\n",
    "\n",
    "- Comet offers data scientists and teams a self-hosted and cloud-based meta machine learning platform that enables tracking, comparing, explanatory, and model optimization.\n",
    "\n",
    "- Comet offers insights and data to construct stronger, more accurate AI models while enhancing productivity, collaboration, and visibility across teams. Comet is supported by thousands of users and several Fortune 100 firms.\n",
    "- Comet will be used by us to version control our research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"COMET_URL_OVERRIDE\"] = \"https://www.comet.com/clientlib/\"\n",
    "from comet_ml import Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an experiment with your api key\n",
    "experiment = Experiment(\n",
    "    api_key=\"bEFY9Hn1QccermEDT6aTyQMOA\",\n",
    "    project_name=\"jm4\",\n",
    "    workspace=\"kojosbk\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id=\"two\"></a>\n",
    "## 2. Import Necessary Libraries\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Import necessary libraries ⚡ |\n",
    "| :--------------------------- |\n",
    "| We'd be importing all of the necessary libraries for the notebook to run smoothly..|\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for importing and loading data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize, TreebankWordTokenizer\n",
    "import scipy as sp # <-- The sister of Numpy, used in our code for numerical efficientcy. \n",
    "import seaborn as sns\n",
    "from textwrap import wrap\n",
    "import pickle\n",
    "\n",
    "#visualization libraries\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "# Entity featurization and similarity computation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Performance Evaluation\n",
    "from surprise import accuracy\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from surprise.model_selection import GridSearchCV, cross_validate, train_test_split\n",
    "\n",
    "# Data Preprocessing\n",
    "import random\n",
    "from time import time\n",
    "import cufflinks as cf\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.ticker import NullFormatter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "# Models\n",
    "from surprise import Reader, Dataset\n",
    "from surprise import SVD, NormalPredictor, BaselineOnly, NMF, SlopeOne, CoClustering\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "# Libraries used during sorting procedures.\n",
    "import operator # <-- Convienient item retrieval during iteration \n",
    "import heapq # <-- Efficient sorting of large lists\n",
    "\n",
    "# Setting global constants to ensure notebook results are reproducible\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Three\"></a>\n",
    "## 3. Loading the Data\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Loading the data ⚡ |\n",
    "| :--------------------------- |\n",
    "| The data from the `train` file is loaded into a DataFrame in this section.. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Train and test data:\n",
    "For training and Kaggle submission purposes, this data is used. The target variable is rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "#Note : the data souce is saved in another directory of the local machine  to avoid github Push issues\n",
    "train = pd.read_csv('C:/Users/Silas_Dell/Documents/Programming/Explore/Unsupervised learning/data/train.csv')\n",
    "test = pd.read_csv('C:/Users/Silas_Dell/Documents/Programming/Explore/Unsupervised learning/data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the  Movie Data\n",
    "#Note : the Movie Data data souce is saved in another directory of the local machine  to avoid github Push issues\n",
    "genome_scores = pd.read_csv('C:/Users/Silas_Dell/Documents/Programming/Explore/Unsupervised learning/data/genome_scores.csv', index_col='movieId')\n",
    "genome_tags = pd.read_csv('C:/Users/Silas_Dell/Documents/Programming/Explore/Unsupervised learning/data/genome_tags.csv')\n",
    "imdb_data = pd.read_csv('C:/Users/Silas_Dell/Documents/Programming/Explore/Unsupervised learning/data/imdb_data.csv')\n",
    "links = pd.read_csv('C:/Users/Silas_Dell/Documents/Programming/Explore/Unsupervised learning/data/links.csv')\n",
    "movies = pd.read_csv('C:/Users/Silas_Dell/Documents/Programming/Explore/Unsupervised learning/data/movies.csv')\n",
    "tags = pd.read_csv('C:/Users/Silas_Dell/Documents/Programming/Explore/Unsupervised learning/data/tags.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=three1></a>\n",
    "\n",
    "#### 3.1 Set Pandas to enable viewing of all columns\n",
    "Due to the length of th content of the message column, pandas cannot display all of them at once by default. While doing EDA and data cleansing, we will need to see all of the columns. When the dataframe is presented, the code below allows us to see the whole set of columns in our data collection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set option to display all columns\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=three2></a>\n",
    "\n",
    "#### 3.2 Check the \"Shape\" of the data-sets\n",
    "As demonstrated by the shape of both datasets, the data has been separated into two sets. The form also shows that the training data set has four columns, but the test data set has just two. Our model is designed to forecast the column that is named movieID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview train dataset\n",
    "print('The Shape of the data is: ', train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be noticed that the train data also includes a column called \"timestamp.\" This information may be safely ignored because there is no connection between when someone watches a movie and whether or not they like it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview test dataset\n",
    "print('The Shape of the data is: ', test.shape)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview genome_scores dataset\n",
    "print('The Shape of the data is: ', genome_scores.shape)\n",
    "genome_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview genome_tags dataset\n",
    "print('The Shape of the data is: ', genome_tags.shape)\n",
    "genome_tags.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview imdb_data dataset\n",
    "print('The Shape of the data is: ', imdb_data.shape)\n",
    "imdb_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview links dataset\n",
    "print('The Shape of the data is: ', links.shape)\n",
    "links.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview movies dataset\n",
    "print('The Shape of the data is: ', movies.shape)\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview tags dataset\n",
    "print('The Shape of the data is: ', tags.shape)\n",
    "tags.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"five\"></a>\n",
    "## 4. Exploratory Data Analysis (EDA)\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Exploratory data analysis ⚡ |\n",
    "| :--------------------------- |\n",
    "| In order to achieve the needed answers, uncover trends, patterns, and correlations that are not immediately obvious, or to get insights into the dataset, we will be analyzing and studying the data sets in this part. |\n",
    "\n",
    "---\n",
    "\n",
    "Exploratory data analysis is a way of studying data sets to highlight their key features, frequently using visual techniques. The crucial process of conducting early investigations on data using summary statistics and graphical representations to find trends, identify anomalies, test hypotheses, and verify assumptions.\n",
    "\n",
    "EDA is primarily used to see what the data can tell us beyond the formal modeling or hypothesis testing work, regardless of whether a statistical model is utilized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=four1></a>\n",
    "\n",
    "#### 5.1 Dataset summary\n",
    "It is important to identify the columns that have null entries as null values can affect the performance of our model. The \"isnull\" function shows the number of null values that are contained in each column of the dataset. This data set is relatively clean \n",
    "Pandas dataframe.info() function is used to get a concise summary of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Summary(df):\n",
    "    i = df.info()\n",
    "    print (\"NUL Values\")\n",
    "    n = df.isna().sum()\n",
    "    return i,n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Summary(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Summary(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Summary(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Summary(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Summary(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Summary(imdb_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Summary(genome_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Summary(genome_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "The Dtype of int64 and float64, which is a sign of numeric values, is present in the majority of DataFrames. However, some DataFrames additionally feature an object's Dtype, which indicates a character other than a numeric character. The DataFrames tags df, movies df, imdb df, and genome df all have a Dtype."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.046907,
     "end_time": "2020-12-08T11:05:12.858703",
     "exception": false,
     "start_time": "2020-12-08T11:05:12.811796",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 5.2 Visualizing the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.069418,
     "end_time": "2020-12-08T11:05:12.976647",
     "exception": false,
     "start_time": "2020-12-08T11:05:12.907229",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Created a Data Frame outlining the size of our data\n",
    "dataframes = ['train', 'test', 'tags', 'imdb',\n",
    "              'links', 'movies', 'genome_tags', 'genome_score']\n",
    "sizes = [(train.shape[0]), (test.shape[0]), (tags.shape[0]),\n",
    "         (imdb_data.shape[0]), (links.shape[0]), (movies.shape[0]),\n",
    "         (genome_tags.shape[0]), (genome_scores.shape[0])]\n",
    "total_size = pd.DataFrame(list(zip(dataframes, sizes)),\n",
    "                             columns=['dataframe', 'sizes'])\n",
    "total_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_list = [['train', (train.shape[0])], ['tags', (tags.shape[0])],\n",
    "            ['imdb', (imdb_data.shape[0])], ['links', (links.shape[0])],\n",
    "            ['movies', (movies.shape[0])],\n",
    "            ['genome_tags', (genome_tags.shape[0])],\n",
    "            ['genome_score', (genome_scores.shape[0])]]\n",
    "len = pd.DataFrame(len_list,\n",
    "                      columns=['Dataset', 'Size'])\n",
    "len = len.sort_values(by='Size', ascending=False)\n",
    "fig, ax = plt.subplots( figsize=(12,6))\n",
    "ax = sns.barplot(y =  len[\"Size\"], x = len[\"Dataset\"], palette='copper')\n",
    "for p in ax.patches:\n",
    "        ax.text(p.get_x() + p.get_width()/2., p.get_height(), '%d' % int(p.get_height()), fontsize=11, ha='center', va='bottom')\n",
    "ax.set_xlabel(\"Data set Categories\", fontsize = 15)\n",
    "ax.set_ylabel(\"SIze of data set\", fontsize = 15)\n",
    "ax.set_title(\"Distribution of overall Data Frames\", fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_list = [['train', (train.shape[0])], ['tags', (tags.shape[0])],\n",
    "            ['imdb', (imdb_data.shape[0])], ['links', (links.shape[0])],\n",
    "            ['movies', (movies.shape[0])],\n",
    "            ['genome_tags', (genome_tags.shape[0])],\n",
    "            ['genome_score', (genome_scores.shape[0])]]\n",
    "len = pd.DataFrame(len_list,\n",
    "                      columns=['Dataset', 'Size'])\n",
    "fig = px.treemap(len, title='Treemap chart by Distribution of overall Data Frames',\n",
    "                 path=[\"Dataset\"], values = 'Size',color='Size', color_continuous_scale=px.colors.sequential.GnBu)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "The precise asymmetry in DataFrame size distribution is shown in the bar graph. The biggest DataFrame is the genome score DataFrame, followed by the train Dataframe. Whereas the bars in the other DataFrame are not readily visible due to the significant disparity in the dimensions, the difference in distribution sizes is obvious as shown in the treemap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Movies produced per Year\n",
    "In this section, we're curious to learn which years had the greatest amount of films made. We may use this information to graphically examine historical performance of the film business."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of movie table\n",
    "movies_table = movies.copy()\n",
    "\n",
    "# Remove delimiters from interested columns\n",
    "movies_table[\"genres\"] = movies_table[\"genres\"].str.replace('|', ' ', regex=True)\n",
    "movies_table[\"title\"] = movies_table[\"title\"].str.replace('(', ' ', regex=True)\n",
    "movies_table[\"title\"] = movies_table[\"title\"].str.replace('$', ' ', regex=True)\n",
    "movies_table[\"title\"] = movies_table[\"title\"].str.replace(')', ' ', regex=True)\n",
    "movies_table[\"genres\"] = movies_table[\"genres\"].str.replace('(', ' ', regex=True)\n",
    "movies_table[\"genres\"] = movies_table[\"genres\"].str.replace(')', ' ', regex=True)\n",
    "# Create a movie year column\n",
    "movies_table[\"year\"] = movies_table[\"title\"].str.replace(r'[a-zA-Z]', '', regex=True)\n",
    "movies_table[\"year\"] = movies_table[\"year\"].str.replace(r' ', '', regex=True)\n",
    "movies_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the first twenty years with the highest numbers of movies produced\n",
    "movies_table[\"year\"] = movies_table[\"year\"].astype('str')\n",
    "movies_year_count = movies_table[\"year\"].value_counts()\n",
    "fig, ax = plt.subplots( figsize=(12,6))\n",
    "ax = sns.barplot(y =  movies_year_count.values[:20], x = movies_year_count.index[:20], palette='copper')\n",
    "ax.set_xlabel(\"Years of Production\", fontsize = 15)\n",
    "ax.set_ylabel(\"Numbers of Movies Produced\", fontsize = 15)\n",
    "ax.set_title(\"First Twent Years with the Highest Numbers of Movies produced\", fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick Observations\n",
    "* The year 2015 had the largest number of films created, with over 1700. It was followed by the years 2016 and 2017, which both saw over 1500 films produced.\n",
    "\n",
    "* One thing to note is that a year like 2019 would have been expected to have a lot of movies released, but due to the outbreak of COVID 19, we observe a decline in movie releases in the year 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Top Rated Movies\n",
    "\n",
    "For this area, we need to know which films have received a rating of 4 or above. Less than a 4 was seen to be an average rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_table = train.copy()\n",
    "train_table.drop(columns='timestamp', inplace=True)\n",
    "ratings = train_table[['movieId','rating']]\n",
    "ratings = ratings[ratings['rating'] > 3.9]\n",
    "ratings = ratings.merge(movies, on = 'movieId', how= 'left')\n",
    "\n",
    "movies_ratings = ratings['title'].value_counts()\n",
    "\n",
    "labels = [ '\\n'.join(wrap(l, 30)) for l in movies_ratings.index[:20]]\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots( figsize=(12,6))\n",
    "ax = sns.barplot(y =  movies_ratings.values[:20], x = labels, palette='copper')\n",
    "ax.set_xlabel(\"Movie Title\", fontsize = 20)\n",
    "ax.set_ylabel(\"Numbers of Ratings\", fontsize = 20)\n",
    "ax.set_title(\"Top Twenty Rated Movies\", fontsize = 30)\n",
    "plt.xticks(rotation=90, fontsize= 7.7)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### key point to note\n",
    "\n",
    "<p float=\"left\">\n",
    "  <img src=\"https://www.themoviedb.org/t/p/w500/q6y0Go1tsGEsmtFryDOJo3dEmqu.jpg\" width=\"300\" height = 400/>\n",
    "  <img src=\"https://cdn.europosters.eu/image/750/posters/pulp-fiction-group-i1295.jpg\" width=\"300\" height = 400/>\n",
    "  <img src=\"https://cps-static.rovicorp.com/2/Rights%20Managed/Belgacom/Forrest%20Gump/_derived_jpg_q90_310x470_m0/ForrestGump_EN.jpg\" width=\"300\" height = 400/>\n",
    "</p>\n",
    "\n",
    "\n",
    "**Top three Most Rated Movies**\n",
    "- Shawshank Redemption (1994)\n",
    "- Pulp Fiction (1994)\n",
    "- Forrest Gump (1994)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_directors(df, count = 10):\n",
    "    \"\"\"\n",
    "    The most frequent directors in a DataFrame may be counted using this function.:\n",
    "    Parameters\n",
    "    ----------\n",
    "        df (DataFrame): dataframe with imdb metadata as input\n",
    "        count (int): directors with fewer than count films to be filtered\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        directors (DataFrame): output DataFrame\n",
    "\n",
    "    \"\"\"\n",
    "    directors = pd.DataFrame(df['director'].value_counts()).reset_index()\n",
    "    directors.columns = ['director', 'count']\n",
    "    # Lets only take directors who have made 10 or more movies otherwise we will have to analyze 11000 directors\n",
    "    directors = directors[directors['count']>=count]\n",
    "    return directors.sort_values('count', ascending = False)\n",
    "directors = count_directors(imdb_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "drt = directors.copy()\n",
    "# drt.drop(drt.index[0], inplace=True)\n",
    "ax = sns.barplot(y = drt[\"director\"].head(10), x = drt['count'], palette='copper', orient='h', edgecolor=\"black\")\n",
    "plt.title(\"Number of Movies Per director\", fontsize=14)\n",
    "plt.ylabel(\"director\")\n",
    "plt.xlabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 3 Directors with most movies released\n",
    "\n",
    "<img height = \"238\" width = 178 src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/81/Luc_Besson_by_Gage_Skidmore.jpg/640px-Luc_Besson_by_Gage_Skidmore.jpg\" alt=\"Photo of Luc Besson\" class=\"GeneratedImage\">  <img height = \"238\" width = 950 src=\"img\\luc.JPG\" alt=\"Movies of Tom Hanks\" class=\"GeneratedImage\">\n",
    "</br>\n",
    "<a href=\"https://en.wikipedia.org/wiki/Luc_Besson\">Luc Paul Maurice Besson</a>  is a French filmmaker, writer, and producer of movies. The Big Blue, La Femme Nikita, and Subway were all movies he either directed or produced. </a>\n",
    "</br>\n",
    "</br>\n",
    "<img height = \"238\" width = 178 src=\"https://encrypted-tbn2.gstatic.com/images?q=tbn:ANd9GcQXYKDvhxIVt8R_yV3LLLZJ2LemcV860GqEgu9TKCDvGSDnHksM\" alt=\"Photo of woody allen\" class=\"GeneratedImage\"> <img height = \"238\" width = 950 src=\"img\\woody.JPG\" alt=\"Movies of Tom Hanks\" class=\"GeneratedImage\">\n",
    "</br>\n",
    "<a href=\"https://en.wikipedia.org/wiki/Woody_Allen\"> Woody Allen</a> is an American filmmaker, writer, actor, and comedian whose career spans more than six decades including several films that have won Academy Awards. </a>\n",
    "</br>\n",
    "</br>\n",
    "\n",
    "<img height = \"238\" width = 178 src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQ3ie6FvZbpJx2VbSMzbGsFagq2wPgnNXJxSPQVTI6ofqhWv28AZKCUZIt54kEQHr9gfiI&usqp=CAU\" alt=\"Photo of Stephen King\" class=\"GeneratedImage\"> <img height = \"238\" width = 950 src=\"img\\king.JPG\" alt=\"Movies of Tom Hanks\" class=\"GeneratedImage\"></br>\n",
    "<a href=\"https://en.wikipedia.org/wiki/Stephen_King\">Stephen King</a>  is an American writer of books in the genres of horror, science fiction, fantasy, suspense, and paranormal fiction. </a>\n",
    "</br>\n",
    "</br>\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Once again we need to calculate a mean rating for each director in order to determine who is the most popular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dir_mean(df):\n",
    "    df.set_index('director', inplace=True)\n",
    "\n",
    "    direct_ratings = []\n",
    "    directors_eda = train.merge(imdb_data, how = 'left')\n",
    "    for director in df.index:\n",
    "        rating = round(directors_eda[directors_eda['director']==director]['rating'].mean(),2)\n",
    "        direct_ratings.append(rating)\n",
    "    df['mean_rating'] = direct_ratings\n",
    "    return df.sort_values('mean_rating', ascending = False)\n",
    "directors = dir_mean(drt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_popularity(df, title = 'feat'):\n",
    "    \"\"\"\n",
    "    Plots the mean rating per director.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plot_data = df.copy()\n",
    "    mean = plot_data['mean_rating'].mean()\n",
    "    min_ = plot_data['mean_rating'].min()\n",
    "    max_ = round(plot_data['mean_rating'].max(),2)\n",
    "    sns.barplot(y = plot_data.index, x = plot_data['mean_rating'], order = plot_data.index, orient='h',palette='copper',edgecolor=\"black\")\n",
    "    plt.axvline(x=mean, label = f'mean {round(mean,1)}' , color='black', lw=1, ls ='--')\n",
    "    plt.axvline(x=min_, label = f'min {round(min_,1)}' , color='#4D17A0', lw=1, ls = '--')\n",
    "    plt.axvline(x=max_, label = f'max {max_}' , color='#4DA017', lw=1,ls = '--')\n",
    "    plt.title(f'Mean Rating Per {title}', fontsize=14)\n",
    "    plt.ylabel(f'{title}')\n",
    "    plt.xlabel('Mean Rating')\n",
    "    plt.legend(loc='lower center')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_popularity(directors.head(10), 'Director')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 3 Directors with High ratings\n",
    "\n",
    "<img height = \"238\" width = 178 src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQ3ie6FvZbpJx2VbSMzbGsFagq2wPgnNXJxSPQVTI6ofqhWv28AZKCUZIt54kEQHr9gfiI&usqp=CAU\" alt=\"Photo of Stephen King\" class=\"GeneratedImage\"> <img height = \"238\" width = 950 src=\"img\\king.JPG\" alt=\"Movies of Tom Hanks\" class=\"GeneratedImage\"></br>\n",
    "<a href=\"https://en.wikipedia.org/wiki/Stephen_King\">Stephen King</a>  is an American writer of books in the genres of horror, science fiction, fantasy, suspense, and paranormal fiction. </a>\n",
    "</br>\n",
    "</br>\n",
    "<img height = \"238\" width = 178 src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQxlTrJvdxqSMBYf90USQe0qXEaMhXdy35FJOpUlEZ5PGl4wIBI\" alt=\"Photo of Quentin Tarantino\" class=\"GeneratedImage\">  <img height = \"238\" width = 950 src=\"img\\quent.JPG\" alt=\"Movies of Tom Hanks\" class=\"GeneratedImage\"></br>\n",
    "</br>\n",
    "<a href=\"https://en.wikipedia.org/wiki/Quentin_Tarantino\"> Quentin Tarantino</a> is an American director, actor, critic, novelist, and screenwriter. His movies are known for their extensive allusions to pop culture and movie history, nonlinear plots, grim comedy, stylised violence, long dialogue, widespread use of profanity, cameos, and ensemble casts. </a>\n",
    "</br>\n",
    "</br>\n",
    "\n",
    "<img height = \"238\" width = 178 src=\"https://encrypted-tbn3.gstatic.com/images?q=tbn:ANd9GcSFu6ohQVsNtOaLKFv6Qv3Xbp2GCIx54HXeTKy0qnVOiZEp4IFT\" alt=\"Photo of John Sayles\" class=\"GeneratedImage\">  <img height = \"238\" width = 950 src=\"img\\jon.png\" alt=\"Movies of Tom Hanks\" class=\"GeneratedImage\"></br>\n",
    "<a href=\"https://en.wikipedia.org/wiki/Stephen_King\">John Sayles</a>   is an American independent film director, screenwriter, editor, actor, and novelist. He has twice been nominated for the Academy Award for Best Original Screenplay, for Passion Fish and Lone Star. His film Men with Guns was nominated for the Golden Globe for Best Foreign Language Film.</a>\n",
    "</br>\n",
    "</br>\n",
    "\n",
    "### Key observatios\n",
    "* From the list above, we can see certain directors who are immediately recognized. Stephen King and Quentin Tarantino are, predictably, at the top of the list.\n",
    "* It comes as no surprise that the director of the film with the highest rating, Shawshank Redemption, is ranked first.\n",
    "* The fact that the top 3 directors have an average mean rating of 4.0 further demonstrates how positively moviegoers rank their favorite films.\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Again, in order to identify which directors are the least well-liked in terms of ratings, we must compute the mean rating for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_popularity(directors.tail(3), 'Director')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 3 Directors with the lowest ratings\n",
    "\n",
    "<img height = \"125\" width = 125 src=\"https://www.thepitchkc.com/content/uploads/2021/11/g/r/charles-band-scaled-e1637002282929.jpg\" alt=\"Photo of Charles Band\" class=\"GeneratedImage\"></br>\n",
    "<a href=\"https://en.wikipedia.org/wiki/Charles_Band\">Charles Band</a> is an American film producer and director, known for his work on horror comedy movies. </a>\n",
    "</br>\n",
    "</br>\n",
    "<img height = \"125\" width = 125 src=\"https://upload.wikimedia.org/wikipedia/en/e/e3/John_Hughes_Home_Alone_2.jpg\" alt=\"Photo of John Hughes\" class=\"GeneratedImage\"> \n",
    "</br>\n",
    "<a href=\"https://en.wikipedia.org/wiki/John_Hughes_(filmmaker)\"> John Hughes</a> was an American filmmaker. Hughes began his career in 1970 as an author of humorous essays and stories for the National Lampoon magazine. </a>\n",
    "</br>\n",
    "</br>\n",
    "\n",
    "<img height = \"125\" width = 125 src=\"https://encrypted-tbn2.gstatic.com/images?q=tbn:ANd9GcQq8v6TWEuFEzwzeUmeVXsHWsR2nTst1wxrbHrSS77qnJMYtqog\" alt=\"Photo of Clive Barker\" class=\"GeneratedImage\"></br>\n",
    "<a href=\"https://en.wikipedia.org/wiki/Clive_Barker\">Clive Barker</a>   is an English playwright, author, film director and visual artist who came to prominence in the mid-1980s with a series of short stories, the Books of Blood, which established him as a leading horror writer. He has since written many novels and other works. </a>\n",
    "</br>\n",
    "</br>\n",
    "\n",
    "### Key observatios\n",
    "* From the list above, we can notice a few filmmakers that stand out right away. Puppet Master was created by Charles band, which gets very poor reviews.\n",
    "\n",
    "* The least popular directors have an average mean rating of 2.5, which further exemplifies how poorly viewers evaluate their movies.\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this code takes over 97min to run for an i5 8th gen 12gig ram on a secon run it took\n",
    "#67min.\n",
    "\"\"\"\n",
    "The most frequent Actors in a DataFrame may be counted using this code.:\n",
    "\"\"\"\n",
    "# Creat a dict to store values\n",
    "im = imdb_data.copy()\n",
    "im = im.dropna(axis=0)\n",
    "cast_dict = {'title_cast': list(),\n",
    "                'count': list(),}\n",
    "# Retrieve a list of all possible cast\n",
    "print('retrieving features...')\n",
    "for imdb in range((im.shape[0])):\n",
    "    cast = im['title_cast'].iloc[imdb].split('|')\n",
    "    for cas in cast:\n",
    "        if cas not in cast_dict['title_cast']:\n",
    "            cast_dict['title_cast'].append(cas)\n",
    "# count the number of occurences of each cast\n",
    "print('counting...')\n",
    "for castt in cast_dict['title_cast']:\n",
    "    count = 0\n",
    "    for imdb in range((im.shape[0])):\n",
    "        cast = im['title_cast'].iloc[imdb].split('|')\n",
    "        if castt in cast:\n",
    "            count += 1\n",
    "    cast_dict['count'].append(count)\n",
    "    \n",
    "    # Calculate metrics\n",
    "cast = pd.DataFrame(cast_dict)\n",
    "print('done!')\n",
    "df = cast.sort_values(by = 'count', ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Actors per movie data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "ax = sns.barplot(y = df['title_cast'].head(10), x = df['count'], palette='copper', orient='h',edgecolor=\"black\")\n",
    "plt.title('Number of Movies Per Actor For Top 10 Actors', fontsize=14)\n",
    "plt.ylabel('Actors')\n",
    "plt.xlabel('Number of movies')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 3 Movie Actors with Highest Number of movies released\n",
    "\n",
    "<img height = \"238\" width = 178 src=\"https://cdn.britannica.com/77/191077-050-63262B99/Samuel-L-Jackson.jpg\" alt=\"Photo of Tom Hanks\" class=\"GeneratedImage\">\n",
    "<img height = \"238\" width = 950 src=\"img\\sam.JPG\" alt=\"Movies of Tom Hanks\" class=\"GeneratedImage\"></br>\n",
    "<a href=\"https://en.wikipedia.org/wiki/Tom_Hanks\">Samuel Leroy Jackson </a>  is an American actor. One of the most widely recognized actors of his generation, the films in which he has appeared have collectively grossed over $27 billion worldwide, making him the highest-grossing actor of all time. </a>\n",
    "</br>\n",
    "</br>\n",
    "<img height = \"238\" width = 178 src=\"https://encrypted-tbn1.gstatic.com/images?q=tbn:ANd9GcSItxlkc_a8e3O3T59cqB6Uw5iPRY5bJlmr8ZUt0KRPLObKcdTd\" alt=\"Photo of Edward Norton\" class=\"GeneratedImage\">\n",
    "<img height = \"238\" width = 950 src=\"img\\wilss.jpg\" alt=\"Movies of Bruce Willis\" class=\"GeneratedImage\"></br>\n",
    "<a href=\"https://en.wikipedia.org/wiki/Bruce_Willis\">Bruce Willis</a>  is a famous American actor. In the 1970s, his acting career got its start on an off-Broadway theater. He rose to stardom in a starring role on the comedy-drama series Moonlighting (1985–1989), and he went on to feature in more movies, being known as an action hero for his performances as John McClane in the Die Hard trilogy (1988–2013) and other projects.</a>\n",
    "</br>\n",
    "</br>\n",
    "\n",
    "<img height = \"238\" width = 178 src=\"https://encrypted-tbn1.gstatic.com/images?q=tbn:ANd9GcRpa6S_5nG_DBliLGNbMMqx_tSAxqmQqDbK26PsKIdZUPxZT017\" alt=\"Photo of Steve Buscemi\" class=\"GeneratedImage\">\n",
    "<img height = \"238\" width = 950 src=\"img\\ste.jpg\" alt=\"Movies of Leonardo DiCaprio\" class=\"GeneratedImage\"></br>\n",
    "<a href=\"https://en.wikipedia.org/wiki/Steve_Buscemi\">Steve Buscemi</a> is an American actor and film producer. Known for his work as a leading man in biopics and period films, he is the recipient of numerous accolades, including an Academy Award, a British Academy Film Award, and three Golden Globe Awards.  </a>\n",
    "</br>\n",
    "</br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key point to consider\n",
    "- Samule l. Jackson, Bruce wills, and Steve Buscemi are the top 3 actors with the most movies released.\n",
    "\n",
    "- Samule l. Jackson has the most realsed movies at 69\n",
    "\n",
    "---\n",
    "\n",
    "### The popularity of the actors is not shown by the above chart, therefore let's compute a mean rating and add it to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_calc(feat_df, ratings = train, movies = imdb_data, column = 'title_cast'):\n",
    "    \"\"\"\n",
    "    Function to add mean ratings to a DataFrame:\n",
    "    Parameters\n",
    "    ----------\n",
    "        feat_df (DataFrame): input dataframe containing cast data\n",
    "        ratings (DataFrame): input dataframe containing ratings data\n",
    "        movies  (DataFrame): input dataframe containing imdb metadata\n",
    "        column  (object): input colum containing title cast\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        Mean ratings (DataFrame): output DataFrame\n",
    "\n",
    "    \"\"\"\n",
    "    mov = movies.copy()\n",
    "    mean_ratings = pd.DataFrame(ratings.merge(mov, how='left').groupby(['movieId'])['rating'].mean())\n",
    "    movie_eda = movies.copy()\n",
    "    movie_eda = movie_eda.join(mean_ratings, on = 'movieId', how = 'left')\n",
    "\n",
    "    # Exclude missing values\n",
    "    movie_eda = movie_eda\n",
    "    movie_eda2 = movie_eda[movie_eda['rating'].notnull()]\n",
    "\n",
    "    means = []\n",
    "    for feat in feat_df[f'{column}']:\n",
    "        mean = round(movie_eda2[movie_eda2[f'{column}'].str.contains(feat)]['rating'].mean(),2)\n",
    "        means.append(mean)\n",
    "    return means\n",
    "cast['mean_rating'] = mean_calc(cast)\n",
    "#sorting values via mean ratings\n",
    "cast.sort_values('mean_rating', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genre_popularity(df):\n",
    "    \"\"\"\n",
    "    Plots the mean rating per genre.\n",
    "    Parameters\n",
    "    ----------\n",
    "        df (DataFrame): input dataframe containing cast data       \n",
    "    Returns\n",
    "    -------\n",
    "        Graph (histogram): output histogram graph\n",
    "    \"\"\"\n",
    "    count_filt = 20\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plot_data = df[df['count']>count_filt].head(10)\n",
    "    mean = plot_data['mean_rating'].mean()\n",
    "    min_ = plot_data['mean_rating'].min()\n",
    "    max_ = plot_data['mean_rating'].max()\n",
    "    sns.barplot(y = plot_data['title_cast'], x = plot_data['mean_rating'], order = plot_data['title_cast'].head(10), orient='h',palette='copper')\n",
    "    plt.axvline(x=mean, label = f'mean {round(mean,1)}' , color='blue', lw=1, ls ='--')\n",
    "    plt.axvline(x=min_, label = f'min {round(min_,1)}' , color='#FF5D32', lw=1, ls = '--')\n",
    "    plt.axvline(x=max_, label = f'max {max_}' , color='r', lw=1,ls = '--')\n",
    "    plt.title(f'Mean Rating Per Actor (20+ movies released)', fontsize=14)\n",
    "    plt.ylabel('Actors')\n",
    "    plt.xlabel('Mean Rating')\n",
    "    plt.legend(loc='lower center')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_popularity(cast.sort_values('mean_rating', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 3 Movie Actors with High ratings (20+ movies released)\n",
    "\n",
    "<img height = \"238\" width = 178 src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS-bt7_ZKFMU8UOOjlq-GYF5P0iNJVuqz9HuDI3GkLmLXDfifpy\" alt=\"Photo of Tom Hanks\" class=\"GeneratedImage\">\n",
    "<img height = \"238\" width = 950 src=\"img\\2tom_h.JPG\" alt=\"Movies of Tom Hanks\" class=\"GeneratedImage\"></br>\n",
    "<a href=\"https://en.wikipedia.org/wiki/Tom_Hanks\">Tom Hanks</a> is an American actor and filmmaker. Known for both his comedic and dramatic roles, he is one of the most popular and recognizable film stars worldwide, and is regarded as an American cultural icon. </a>\n",
    "</br>\n",
    "</br>\n",
    "<img height = \"238\" width = 178 src=\"https://encrypted-tbn3.gstatic.com/images?q=tbn:ANd9GcTWkkwUYKcdK296LMj5-aAkieIrvt7pqfHc4N16BIvL9EHFSpuf\" alt=\"Photo of Edward Norton\" class=\"GeneratedImage\">\n",
    "<img height = \"238\" width = 950 src=\"img\\tempsnip.png\" alt=\"Movies of Edward Norton\" class=\"GeneratedImage\"></br>\n",
    "<a href=\"https://en.wikipedia.org/wiki/Edward_Norton\">Edward Norton</a> is an American actor and filmmaker. He has received numerous awards and nominations, including a Golden Globe Award and three Academy Award nominations. Born in Boston, Massachusetts and raised in Columbia, Maryland, Norton was drawn to theatrical productions at local venues as a child. </a>\n",
    "</br>\n",
    "</br>\n",
    "\n",
    "<img height = \"238\" width = 178 src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR2q9tvih6sHPAEEbPoCRrWpf2IWVG5IOo5jIxqCA7dgrggsQO5\" alt=\"Photo of Leonardo DiCaprio\" class=\"GeneratedImage\">\n",
    "<img height = \"238\" width = 950 src=\"img\\leo.jpg\" alt=\"Movies of Leonardo DiCaprio\" class=\"GeneratedImage\"></br>\n",
    "<a href=\"https://en.wikipedia.org/wiki/Leonardo_DiCaprio\">Leonardo DiCaprio</a> is an American actor and film producer. Known for his work as a leading man in biopics and period films, he is the recipient of numerous accolades, including an Academy Award, a British Academy Film Award, and three Golden Globe Awards.  </a>\n",
    "</br>\n",
    "</br>\n",
    "\n",
    "### Key observatios\n",
    "* As we can see from the list above, practically every actor is easily identifiable. Naturally, Tom Hanks and Leonardo DiCaprio are at the top of the list.\n",
    "\n",
    "* The average mean rating of 3.6 for the top 3 actors further indicates how highly moviegoers regard their favorite actors.\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consumer perception of movie reviews or ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "ax = sns.distplot(train['rating'],bins=10, kde=False, hist_kws=dict(alpha=0.6),color=\"#FF5D32\")\n",
    "mean = train['rating'].mean()\n",
    "median = train['rating'].median()\n",
    "plt.axvline(x=mean, label = f'mean {round(mean,2)}' , color='#FF5D32', lw=3, ls = '--')\n",
    "plt.axvline(x=median, label = f'median {median}' , color='#4DA017', lw=3, ls = '--')\n",
    "plt.xlim((0.5,5))\n",
    "plt.ylim((0,2500000))\n",
    "plt.title(f'Distribution of Ratings', fontsize=14)\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key point to note\n",
    "\n",
    " - The ratings' left-leaning bias is intriguing. An average distribution with a mean rating of 3.5 was anticipated. Instead, we see that viewers often give movies positive reviews and steer clear of giving them bad ones. The inclination of users to rank movies they like may help to explain this bias. In other words, it is unlikely that a person will finish watching and rating a film if they don't enjoy it.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### relationship between the number of ratings a movie has and how highly it is rated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "mean_ratings = train.groupby(\"movieId\")['rating'].mean()\n",
    "user_counts = train.groupby(\"movieId\")['movieId'].count().values\n",
    "sns.scatterplot(x=mean_ratings, y = user_counts, color=\"#FF5D32\", )\n",
    "plt.title(f'Correlation between a movies rating and its rating count', fontsize=14)\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Number of Ratings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the scatter plot shown above, a movie is more likely to receive a high rating the more ratings it has. This supports our innate belief that moviegoers are more inclined to suggest a film to one another if it has a higher rating. In other words, most individuals aim to avoid making unfavorable suggestions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percentage distribution of users per rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movieRatingDistGroup = train['rating'].value_counts().sort_index().reset_index()\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "sns.barplot(data=movieRatingDistGroup, x='index', y='rating', palette=\"copper\", edgecolor=\"black\", ax=ax)\n",
    "ax.set_xlabel(\"Rating\")\n",
    "ax.set_ylabel('Number of Users')\n",
    "ax.set_yticklabels(['{:,}'.format(int(x)) for x in ax.get_yticks().tolist()])\n",
    "total = float(movieRatingDistGroup['rating'].sum())\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    ax.text(p.get_x()+p.get_width()/2., height+350, '{0:.2%}'.format(height/total), fontsize=11, ha=\"center\", va='bottom')\n",
    "plt.title('Number of Users Per Rating', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the bar graph up top that the majority of movies receive a rating of 4.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4 Top Movie Viewers\n",
    "We're interested in finding out which userIds watch the most movies at this point. This information may be used to determine the top clients and their preferred movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top movie viewers\n",
    "train_table['userId'] = train_table['userId'].astype('str')\n",
    "viewers = train_table['userId'].value_counts()\n",
    "fig, ax = plt.subplots( figsize=(10,6))\n",
    "\n",
    "ax = sns.barplot(y = viewers.values[:20], x = viewers.index[:20], palette='copper',edgecolor=\"black\")\n",
    "ax.set_xlabel(\"UserIds\", fontsize = 20)\n",
    "ax.set_ylabel(\"Numbers of Movies\", fontsize = 20)\n",
    "ax.set_title(\"Top Twenty Movie Reviewers\", fontsize = 30)\n",
    "plt.xticks(rotation=90, fontsize= 10)\n",
    "plt.yticks( fontsize= 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Top Movie Viewers by user ID**\n",
    "- 72315\n",
    "- 80974\n",
    "- 137293\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creat a dict to store values\n",
    "movies = movies.dropna(axis=0)\n",
    "genre_dict = {'genres': list(),\n",
    "                'count': list(),}\n",
    "# Retrieve a list of all possible genres\n",
    "print('retrieving features...')\n",
    "for movie in range((movies.shape[0])):\n",
    "    gens = movies['genres'].iloc[movie].split('|')\n",
    "    for gen in gens:\n",
    "        if gen not in genre_dict['genres']:\n",
    "            genre_dict['genres'].append(gen)\n",
    "# count the number of occurences of each genre\n",
    "print('counting...')\n",
    "for genre in genre_dict['genres']:\n",
    "    count = 0\n",
    "    for movie in range((movies.shape[0])):\n",
    "        gens = movies['genres'].iloc[movie].split('|')\n",
    "        if genre in gens:\n",
    "            count += 1\n",
    "    genre_dict['count'].append(count)\n",
    "    \n",
    "    # Calculate metrics\n",
    "genres = pd.DataFrame(genre_dict)\n",
    "print('done!')\n",
    "df = genres.sort_values(by = 'count', ascending=False)\n",
    "plt.figure(figsize=(10,6))\n",
    "ax = sns.barplot(y = df['genres'], x = df['count'], palette='copper', orient='h',edgecolor=\"black\")\n",
    "plt.title('Number of Movies Per genres', fontsize=14)\n",
    "plt.ylabel('genres')\n",
    "plt.xlabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key point to consider\n",
    "- Drama, comedy, and thriller are the top 3 movie genres in popularity.\n",
    "Character, story, storyline, and setting are the four constituent aspects or pieces of a genre. Additionally, it's common to hear individuals say that a certain movie had a solid storyline or an interesting tale. in reality the characters, the issues/conflicts the characters encountered, and the resolution of those issues and conflicts are what people are really referring to. Drama may be the most popular genre since it focuses on character development in the story, frequently overcoming a variety of obstacles and conflicts, or human challenges.\n",
    "\n",
    "- Drama provides the emotional and interpersonal growth of realistic individuals in a realistic context, according to Hayley Mckenzie. It delivers an openly depicted tale of human adversity and provides rich character development. And this may be the main factor behind the popularity of the drama genre.\n",
    "\n",
    "- Approximately 5000 movies have missing genres.\n",
    "\n",
    "- We can use the IMDB and TMDB ID's together with the APIs to fill missing data. Further, IMAX is not a genre but rather a proprietary system for mass-viewings.\n",
    "---\n",
    "\n",
    "### The above figure does not tell us anything about the popularity of the genres, lets calculate a mean rating and append it to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_calc(feat_df, ratings = train, movies = movies, metadata = imdb_data, column = 'genres'):\n",
    "    mean_ratings = pd.DataFrame(ratings.merge(movies, how='left').groupby(['movieId'])['rating'].mean())\n",
    "    movie_eda = movies.copy()\n",
    "    movie_eda = movie_eda.join(mean_ratings, on = 'movieId', how = 'left')\n",
    "\n",
    "    # Exclude missing values\n",
    "    movie_eda = movie_eda\n",
    "    movie_eda2 = movie_eda[movie_eda['rating'].notnull()]\n",
    "\n",
    "    means = []\n",
    "    for feat in feat_df[f'{column}']:\n",
    "        mean = round(movie_eda2[movie_eda2[f'{column}'].str.contains(feat)]['rating'].mean(),2)\n",
    "        means.append(mean)\n",
    "    return means\n",
    "genres['mean_rating'] = mean_calc(genres)\n",
    "genres.sort_values('mean_rating', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genre_popularity(df):\n",
    "    \"\"\"\n",
    "    Plots the mean rating per genre.\n",
    "    \"\"\"\n",
    "    count_filt = 500\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plot_data = df[df['count']>count_filt]\n",
    "    mean = plot_data['mean_rating'].mean()\n",
    "    min_ = plot_data['mean_rating'].min()\n",
    "    max_ = plot_data['mean_rating'].max()\n",
    "    sns.barplot(y = plot_data['genres'], x = plot_data['mean_rating'], order = plot_data['genres'], orient='h',palette='copper')\n",
    "    plt.axvline(x=mean, label = f'mean {round(mean,1)}' , color='blue', lw=1, ls ='--')\n",
    "    plt.axvline(x=min_, label = f'min {round(min_,1)}' , color='#FF5D32', lw=1, ls = '--')\n",
    "    plt.axvline(x=max_, label = f'max {max_}' , color='r', lw=1,ls = '--')\n",
    "    plt.title(f'Mean Rating Per Genre', fontsize=14)\n",
    "    plt.ylabel('Genre')\n",
    "    plt.xlabel('Mean Rating')\n",
    "    plt.legend(loc='lower center')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_popularity(genres.sort_values('mean_rating', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "<img height = \"210\" width = 400 src=\"https://imgix.bustle.com/fatherly/2019/06/best-nature-documentaries.jpg?w=1200&h=630&fit=crop&crop=faces&fm=jpg\" align=\"center\" alt=\"Photo of Clive Barker\" class=\"GeneratedImage\"></br>\n",
    "\n",
    "- Documentries seams to be the most higly rated releases in the data\n",
    "\n",
    "- The ratings are almost evenly distributed, with the exception of documentaries, conflict, drama, musicals, and romance, which score over average. On the other side, the ratings for thriller, action, science fiction, and horror are noticeably below average.\n",
    "\n",
    "- Hollywood crime dramas are referred to as \"film-noir,\" especially those that stress cynical attitudes and sexual desires. In general, the \"classic era\" of American film-noir is thought to have been the 1940s and 1950s. Though it's possible that their particular audience is why some films earn the greatest ratings. For IMAX movies, the same reasoning holds true; hence, we only accounted for categories with a count of 500 or more in this graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_words = ''\n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "# iterate through the csv file\n",
    "for val in tags['tag']:\n",
    "\n",
    "    # typecaste each val to string\n",
    "    val = str(val)\n",
    "\n",
    "    # split the value\n",
    "    tokens = val.split()\n",
    "\n",
    "    # Converts each token into lowercase\n",
    "    for i in range((np.shape(tokens)[0])):\n",
    "        tokens[i] = tokens[i].lower()\n",
    "\n",
    "    comment_words += \" \".join(tokens)+\" \"\n",
    "  \n",
    "wordcloud = WordCloud(width=1200, height=900,\n",
    "                      colormap='copper',\n",
    "                      background_color='white',\n",
    "                      stopwords=stopwords,collocations=False,\n",
    "                      min_font_size=10).generate(comment_words)\n",
    "\n",
    "# plot the WordCloud image\n",
    "plt.figure(figsize=(14, 7), facecolor=\"white\")\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title('Distribution of words in the tags data frame by Tags\\n----------------------------------------------------',fontsize=25,y=1.05 ,loc='center')\n",
    "plt.tight_layout(pad=0)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"four\"></a>\n",
    "## 6. Data Engineering\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Data engineering ⚡ |\n",
    "| :--------------------------- |\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data(Feature) Engineering** is the process of using domain knowledge to reconfigure the data and create “features” that optimize machine learning algorithms. With the insight gained from the **EDA** section above, we have decided to work with the below listed table:\n",
    "- train\n",
    "- test\n",
    "- movies\n",
    "- imdb_data\n",
    "\n",
    "Also, for this phase we shall execute the below task on our dataset:\n",
    "- Identify the unique userIds in the test data and only work with same for the purpose of **Content-based** algorithm\n",
    "- Sort both the train and test dataset by userId for both algorithms\n",
    "- Merge the movie and imdb_data tables separately with both the test and train dataset, for both algorithms \n",
    "- Drop unwanted columns after the merge\n",
    "- Generate a key word column, which is a collection of all vital features of a movie for **conten-based** algorithm\n",
    "- Divide the dataset set into chunks of 162,350( unique userId per chunk), to ease data processing acivities like vectorization and evaluation of cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multidimensional Scaling\n",
    "Multidimensional scaling (MDS) is a technique for visualizing distances between objects on a map, where the distance is known between pairs of the objects.  \n",
    "> \"The t-SNE algorithm comprises two main stages. First, t-SNE constructs a probability distribution over pairs of high-dimensional objects in such a way that similar objects have a high probability of being picked, whilst dissimilar points have an extremely small probability of being picked. Second, t-SNE defines a similar probability distribution over the points in the low-dimensional map, and it minimizes the [Kullback–Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) between the two distributions with respect to the locations of the points in the map. Note that whilst the original algorithm uses the Euclidean distance between objects as the base of its similarity metric, this should be changed as appropriate.\" [[3]](#ref3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the data to cut down computation time for now\n",
    "genome_score = genome_scores[:10000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Although scores are in the range of 0-1, there is no harm in scaling\n",
    "scaler_mds = StandardScaler()\n",
    "mds_genome = scaler_mds.fit_transform(genome_score.sample(frac=0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(3, n_jobs = -1, verbose = 2, perplexity = 10, learning_rate = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne.fit(mds_genome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n6hh2GoxWPxN",
    "outputId": "4f9d18af-7bcf-4104-8bd1-195eb8ad8cdc"
   },
   "outputs": [],
   "source": [
    "Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Add 3D scatter plot\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.scatter(tsne.embedding_[:,0], tsne.embedding_[:,1], tsne.embedding_[:,2], color='#4D17A0')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9400drfVWPxP",
    "outputId": "6b32494e-f5bb-40f1-9ab0-d1663c946329",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x = tsne.embedding_[:,0], y = tsne.embedding_[:,1], size=tsne.embedding_[:,2],color='#4DA017')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WT2hM7v6vThL"
   },
   "source": [
    "### Principal Component Analysis\n",
    "Principal component analysis (PCA) is a technique for reducing the dimensionality of such datasets, increasing interpretability but at the same time minimizing information loss. It does so by creating new uncorrelated variables that successively maximize variance. [[4]](#ref4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SALNYG0BvThM"
   },
   "outputs": [],
   "source": [
    "# Manually pivot table as data is too large for in-built functions\n",
    "def pivot_(df):\n",
    "    \"\"\"\n",
    "    Pivots table.\n",
    "    \"\"\"\n",
    "    new_dict = {'movieId':sorted(set(df.index))}\n",
    "    pivoted = pd.DataFrame(new_dict)\n",
    "    tagids = sorted(set(df['tagId']))\n",
    "    for Id in range(np.shape(tagids)[0]):\n",
    "        pivoted[f'{Id+1}'] = list(df[df['tagId'] == Id+1]['relevance'])\n",
    "    return pivoted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1oUb0R95vThN"
   },
   "outputs": [],
   "source": [
    "\n",
    "pca_data_pivoted = pivot_(genome_scores).set_index('movieId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mvI19CwqvThQ",
    "outputId": "0223f05a-3308-4224-f0b3-dcc08a74e3cd"
   },
   "outputs": [],
   "source": [
    "pca_data_pivoted.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FXeQYBiPvThS"
   },
   "source": [
    "Lets make this more readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B6P-g2WGvThS"
   },
   "outputs": [],
   "source": [
    "pca_data_pivoted.columns = list(genome_tags['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FaKE5iqlvThU",
    "outputId": "7f50eadd-1c02-4005-e196-715a701dd6ba",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pca_data_pivoted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GX12_4-tvThW"
   },
   "outputs": [],
   "source": [
    "features = [col for col in pca_data_pivoted.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1ZIjwVzkvThY"
   },
   "source": [
    "#### Scales of measurement\n",
    "It is important that we scale the data before dimensionality reduction.\n",
    "\n",
    "Although all variables are measured on the same scale (0-1), there shouldn't be any downside to setting the mean to zero and standard deviation to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mJ9xevnQvThZ",
    "outputId": "a7a2598e-bd03-47ef-84b6-e26f9b02243a"
   },
   "outputs": [],
   "source": [
    "cf.set_config_file(offline=True, world_readable=True, theme='white')\n",
    "columns = random.sample(range(0, 1129), 20)\n",
    "pca_data_pivoted.iloc[:,columns].iplot(kind='box', title=\"Boxplots of Features (Unscaled)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y8L9aV1PvThi"
   },
   "outputs": [],
   "source": [
    "def scaler(df):\n",
    "    \"\"\"\n",
    "    Scales data.\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler(with_std=True)\n",
    "    scaled_data = scaler.fit_transform(df)\n",
    "    return scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SaNuJlshvThl"
   },
   "outputs": [],
   "source": [
    "pca_scaled = scaler(pca_data_pivoted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IWprBTLvvThp"
   },
   "outputs": [],
   "source": [
    "scaled_pca = pd.DataFrame(pca_scaled, index = pca_data_pivoted.index, columns = pca_data_pivoted.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LxL2umW0vThr",
    "outputId": "f9b316f5-1de9-4222-ea2d-70bbcbd8b8ee",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cf.set_config_file(offline=True, world_readable=True, theme='white')\n",
    "# using plotly to plot the boxplot\n",
    "scaled_pca.iloc[:,columns].iplot(kind='box', title=\"Boxplots of Features (Scaled)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cQAGbWDmvThy",
    "outputId": "d8bdcc12-1d17-4160-a87a-6d60a95ddd39",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define PCA object\n",
    "pca = PCA()\n",
    "\n",
    "# fit the PCA model to our data and apply the dimensionality reduction \n",
    "prin_comp = pca.fit_transform(pca_data_pivoted[features])\n",
    "\n",
    "# create a dataframe containing the principal components\n",
    "pca_df = pd.DataFrame(data = prin_comp,\n",
    "                      index=pca_data_pivoted.index, columns=pca_data_pivoted.columns\n",
    "                     )\n",
    "\n",
    "# plot line graph of cumulative variance explained\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_),color='#4D17A0')\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Cumulative explained variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u6HbWiYJvTh0",
    "outputId": "09019d96-4b26-47db-c66c-43b0268ef2bd"
   },
   "outputs": [],
   "source": [
    "pca_75 = PCA(.80)\n",
    "pca_75_df = pca_75.fit_transform(pca_data_pivoted)\n",
    "print(round(pca_75.explained_variance_ratio_.sum()*100, 1),\n",
    "      \"% of variance explained by\",\n",
    "      pca_75.n_components_,\n",
    "      \"components.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3ZQihRAblwwO"
   },
   "source": [
    "The cummulative explained variance shows an initially steep then gradual curve and not the sharp elbow we were expecting. This could be a result of the genomes already having been chosen as the principle components of movies. However we can see that 80% of the variance in the movie dataset is explained by 131 components. We should use only these components for computatonal efficiency in a content based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "seD8qOOavTh7",
    "outputId": "97e26824-c588-4561-9315-4ae1156e4efe",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pca_75_df = pd.DataFrame(pca_75_df, index = pca_data_pivoted.index)\n",
    "pca_75_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5UYQuJkwvTh7"
   },
   "source": [
    "### Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nvQgXservTiA"
   },
   "outputs": [],
   "source": [
    "# Manually implement the WCSS\n",
    "def within_cluster_variation(df, label_col='cluster_label'):\n",
    "    \"\"\"\n",
    "    Manually implements the WCSS.\n",
    "    \"\"\"\n",
    "    centroids = df.groupby(label_col).mean()\n",
    "    out = 0\n",
    "    for label, point in centroids.iterrows():\n",
    "        df_features = df[df[label_col] == label].drop(label_col, axis=1)\n",
    "        out += (df_features - point).pow(2).sum(axis=1).sum()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PXK98vYcvTiD",
    "outputId": "8828215a-a495-4019-86c0-6fe7dbbb4eb7"
   },
   "outputs": [],
   "source": [
    "# let's try everything between 2 and 18 clusters where 18 is the number of genres\n",
    "n_clusters = np.arange(2, 19)\n",
    "\n",
    "# store errors for each value of k\n",
    "errors = []\n",
    "\n",
    "# for i between 2 and 19\n",
    "for k in n_clusters:\n",
    "    print(f'training model with {k} clusters')\n",
    "    # perform k-means clustering\n",
    "    km = KMeans(n_clusters=k, n_init=10, max_iter=300, random_state=42)\n",
    "    km.fit(pca_75_df)\n",
    "    \n",
    "    # measure BCSS\n",
    "    print(f'evaluating model with {k} clusters')\n",
    "    y_preds = km.predict(pca_75_df)\n",
    "    pca_75_df = pd.DataFrame(pca_75_df)\n",
    "    pca_75_df['cluster_label'] = y_preds\n",
    "    errors.append(within_cluster_variation(pca_75_df, 'cluster_label'))\n",
    "    print(errors[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nbijk0KBvTiG",
    "outputId": "f33c4d77-7f5a-474f-e5d1-56edb0348dbd"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Within-Cluster Sum of Squares (WCSS)')\n",
    "plt.title('Elbow Method for Determining Optimal Value of k')\n",
    "plt.scatter(n_clusters, errors, color=\"#4DA017\")\n",
    "plt.plot(n_clusters, errors)\n",
    "plt.xticks(n_clusters)\n",
    "plt.axvline(x=3, color='#4D17A0', lw=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NiulgkQqvTiI",
    "outputId": "74486816-626b-49cc-d5a9-fc1a9fbe6a28"
   },
   "outputs": [],
   "source": [
    "K = 3\n",
    "# Remember to set the random state for reproducibility\n",
    "km = KMeans(n_clusters=K, verbose=0, random_state=42)\n",
    "print(\"Clustering sparse data with %s\" % km)\n",
    "t0 = time()\n",
    "km.fit(pca_75_df)\n",
    "print(\"done in %0.3fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RvbTC_dDvTiM"
   },
   "outputs": [],
   "source": [
    "# Obtain cluster memberships for each item in the data\n",
    "y_preds = km.predict(pca_75_df)\n",
    "pca_75_df['cluster_label'] = y_preds\n",
    "centers = km.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sS24QugKvTiP",
    "outputId": "1b7cc16a-729c-48b6-b044-b909280bd2a5",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(dpi=120)\n",
    "for k in range(K):\n",
    "    x1 = pca_75_df[pca_75_df['cluster_label'] == k][0]\n",
    "    x2 = pca_75_df[pca_75_df['cluster_label'] == k][1]\n",
    "    plt.scatter(x1, x2, label=\"k = \"+str(k+1),alpha=0.75)\n",
    "# Show cluster centroid locations    \n",
    "plt.scatter(centers[:,0],centers[:,1],label=\"centroid\")\n",
    "plt.legend()\n",
    "plt.title(f\"K = {K}\")\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "chM0AHBJ2FyD"
   },
   "source": [
    "<a id=\"modelling\"></a>\n",
    "## 5. Modelling\n",
    "\n",
    "To reduce computation time, we train and evaluate the following models on a 100k subset of the data. The best performing model will be trained on the whole dataset to predict the ratings for the final submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "whaHVhWzWPyD"
   },
   "outputs": [],
   "source": [
    "# Load the 100k dataset\n",
    "train.drop('timestamp', axis=1, inplace=True)\n",
    "train_subset = train[:100000]\n",
    "reader = Reader(rating_scale=(train_subset['rating'].min(), train_subset['rating'].max()))\n",
    "data = Dataset.load_from_df(train_subset[['userId', 'movieId', 'rating']], reader)\n",
    "trainset, testset = train_test_split(data, test_size=.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zbe96XXoWPyF"
   },
   "source": [
    "### Collaborative Filtering\n",
    "\n",
    "Collaborative filtering is a technique that can filter out items that a user might like on the basis of reactions by similar users. It works by searching a large group of people and finding a smaller set of users with tastes similar to a particular user [[5]](#ref5).\n",
    "\n",
    "#### SVD\n",
    "\n",
    "The Singular Value Decomposition algorithm is a matrix factorization technique which reduces the number of features of a dataset and was popularized by Simon Funk during the [Neflix Prize](https://en.wikipedia.org/wiki/Netflix_Prize) contest [[6]](#ref6). In the matrix structure, each row represents a user and each column represents a movie. The matrix elements are ratings that are given to movies by users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NFKprc2IWPyG",
    "outputId": "3b7844eb-f2aa-45be-cfbf-c5319bbb10ed"
   },
   "outputs": [],
   "source": [
    "svd_test = SVD(n_epochs = 30, n_factors = 200, init_std_dev = 0.05, random_state=42)\n",
    "svd_test.fit(trainset)\n",
    "predictions = svd_test.test(testset)\n",
    "# Calculate RMSE\n",
    "svd_rmse = accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3_x-FhLNWPyI"
   },
   "source": [
    "#### NormalPredictor  \n",
    "The Normal Predictor algorithm predicts a random rating for each movie based on the distribution of the training set, which is assumed to be normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pG2PARN5WPyI",
    "outputId": "12e482e2-4a8c-4360-d661-0c98532f1b23"
   },
   "outputs": [],
   "source": [
    "np_test = NormalPredictor()\n",
    "np_test.fit(trainset)\n",
    "predictions = np_test.test(testset)\n",
    "# Calculate RMSE\n",
    "np_rmse = accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ONPKskgFWPyK"
   },
   "source": [
    "#### BaselineOnly  \n",
    "The Baseline Only algorithm predicts the baseline estimate for a given user and movie. A baseline is calculated using either Stochastic Gradient Descent (SGD) or Alternating Least Squares (ALS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B__DCfvfWPyK",
    "outputId": "96162adb-d01a-47b1-8fe2-5f4fb0e89aea"
   },
   "outputs": [],
   "source": [
    "bsl_options = {'method': 'sgd','n_epochs': 40}\n",
    "blo_test = BaselineOnly(bsl_options=bsl_options)\n",
    "blo_test.fit(trainset)\n",
    "predictions = blo_test.test(testset)\n",
    "# Calculate RMSE\n",
    "blo_rmse = accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T0hdUFg0WPyL"
   },
   "source": [
    "#### NMF  \n",
    "NMF is a collaborative filtering algorithm based on Non-negative Matrix Factorization. The optimization procedure is a (regularized) stochastic gradient descent with a specific choice of step size that ensures non-negativity of factors, provided that their initial values are also positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "21IiVZyOWPyM",
    "outputId": "d157eef4-3a6c-4691-82c6-e77a6051460a"
   },
   "outputs": [],
   "source": [
    "nmf_test = NMF()\n",
    "nmf_test.fit(trainset)\n",
    "predictions = nmf_test.test(testset)\n",
    "# Calculate RMSE\n",
    "nmf_rmse = accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0rKRlHmDWPyN"
   },
   "source": [
    "#### SlopeOne  \n",
    "The SlopeOne algorithm is a simple yet accurate collaborative filtering algorithm that uses a simple linear regression model to solve the data sparisity problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I_meia6EWPyN",
    "outputId": "6acf4137-ca89-4c01-f3b0-705811b3e443"
   },
   "outputs": [],
   "source": [
    "slo_test = SlopeOne()\n",
    "slo_test.fit(trainset)\n",
    "predictions = slo_test.test(testset)\n",
    "# Calculate RMSE\n",
    "slo_rmse = accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XvlEikIiWPyO"
   },
   "source": [
    "#### CoClustering  \n",
    "The Co-clustering algorithm assigns clusters using a straightforward optimization method, much like k-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-AmTtK0NWPyP",
    "outputId": "caca6344-c5f2-4fe4-f6ad-c8b01e0ec3fc"
   },
   "outputs": [],
   "source": [
    "cc_test = CoClustering(random_state=42)\n",
    "cc_test.fit(trainset)\n",
    "predictions = cc_test.test(testset)\n",
    "# Calculate RMSE\n",
    "cc_rmse = accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zaamIZ82WPyQ"
   },
   "source": [
    "### Content-based Filtering  \n",
    "Content-based filtering, also referred to as cognitive filtering, recommends items based on a comparison between the content of the items and a user profile. The content of each item is represented as a set of descriptors or terms, typically the words that occur in a document [[7]](#ref7). In the following section, the model uses genres as keywords to recommend similar movies based on input from a user. The model was not used to predict ratings for the testing data, as it is too computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FuoTVsX3WPyR"
   },
   "outputs": [],
   "source": [
    "def data_preprocessing(subset_size):\n",
    "    \"\"\"Prepare data for use within Content filtering algorithm.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    subset_size : int\n",
    "        Number of movies to use within the algorithm.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Pandas Dataframe\n",
    "        Subset of movies selected for content-based filtering.\n",
    "\n",
    "    \"\"\"\n",
    "    # Split genre data into individual words.\n",
    "    movies['keyWords'] = movies['genres'].str.replace('|', ' ')\n",
    "    # Subset of the data\n",
    "    movies_subset = movies[:subset_size]\n",
    "    return movies_subset\n",
    " \n",
    "def content_model(movie_list,top_n=10): \n",
    "    \"\"\"Performs Content filtering based upon a list of movies supplied\n",
    "       by the app user.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    movie_list : list (str)\n",
    "        Favorite movies chosen by the app user.\n",
    "    top_n : type\n",
    "        Number of top recommendations to return to the user.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list (str)\n",
    "        Titles of the top-n movie recommendations to the user.\n",
    "\n",
    "    \"\"\"\n",
    "    # Initializing the empty list of recommended movies\n",
    "    data = data_preprocessing(2000)\n",
    "    # Instantiating and generating the count matrix\n",
    "    count_vec = CountVectorizer()\n",
    "    count_matrix = count_vec.fit_transform(data['keyWords'])\n",
    "    indices = pd.Series(data['title'])\n",
    "    cosine_sim = cosine_similarity(count_matrix, count_matrix)\n",
    "    cosine_sim = pd.DataFrame(cosine_sim, index = data.index, columns = data.index)\n",
    "    # Getting the index of the movie that matches the title\n",
    "    idx_1 = indices[indices == movie_list[0]].index[0]\n",
    "    idx_2 = indices[indices == movie_list[1]].index[0]\n",
    "    idx_3 = indices[indices == movie_list[2]].index[0]\n",
    "    # Creating a Series with the similarity scores in descending order\n",
    "    rank_1 = cosine_sim[idx_1]\n",
    "    rank_2 = cosine_sim[idx_2]\n",
    "    rank_3 = cosine_sim[idx_3]\n",
    "    # Calculating the scores\n",
    "    score_series_1 = pd.Series(rank_1).sort_values(ascending = False)\n",
    "    score_series_2 = pd.Series(rank_2).sort_values(ascending = False)\n",
    "    score_series_3 = pd.Series(rank_3).sort_values(ascending = False)\n",
    "    # Getting the indexes of the 10 most similar movies\n",
    "    listings = score_series_1.append(score_series_2).append(score_series_3).sort_values(ascending = False)\n",
    "    # Store movie names\n",
    "    recommended_movies = []\n",
    "    # Appending the names of movies\n",
    "    top_50_indexes = list(listings.iloc[1:50].index)\n",
    "    # Removing chosen movies\n",
    "    top_indexes = np.setdiff1d(top_50_indexes,[idx_1,idx_2,idx_3])\n",
    "    for i in top_indexes[:top_n]:\n",
    "        recommended_movies.append(list(movies['title'])[i])\n",
    "    return recommended_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EAmiLj_RWPyS",
    "outputId": "0293c329-5d82-430a-8cf6-3df1bd828221"
   },
   "outputs": [],
   "source": [
    "movies = movies.dropna()\n",
    "movie_list = ['Grumpier Old Men (1995)','Ace Ventura: When Nature Calls (1995)','Father of the Bride Part II (1995)']\n",
    "content_model(movie_list,top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "teKzAXIF2FyS"
   },
   "source": [
    "<a id=\"evaluation\"></a>\n",
    "## 6. Performance Evaluation\n",
    "\n",
    "We built and tested six different collaborative filtering models and compared their performance using a statistical measure known as the root mean squared error (**RMSE**), which determines the average squared difference between the estimated values and the actual value. A low RMSE value indicates high model accuracy.\n",
    "\n",
    "### Root Mean Squared Error (RMSE):\n",
    "$$RMSE = \\sqrt{\\frac{1}{n}\\Sigma_{i=1}^{n}{\\Big(\\frac{d_i -f_i}{\\sigma_i}\\Big)^2}}$$   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r51HzbnTWPyU",
    "outputId": "1425b9ff-f385-46d9-bc39-f135bb3b2c36"
   },
   "outputs": [],
   "source": [
    "# Compare RMSE values between models\n",
    "fig,axis = plt.subplots(figsize=(8, 5))\n",
    "rmse_x = ['SVD','NormalPredictor','BaselineOnly','NMF','SlopeOne','CoClustering']\n",
    "rmse_y = [svd_rmse,np_rmse,blo_rmse,nmf_rmse,slo_rmse,cc_rmse]\n",
    "ax = sns.barplot(x=rmse_x, y=rmse_y,palette='brg',edgecolor='black')\n",
    "plt.title('RMSE Value Per Collaborative-based Filtering Model',fontsize=14)\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('RMSE')\n",
    "for p in ax.patches:\n",
    "    ax.text(p.get_x() + p.get_width()/2, p.get_y() + p.get_height(), round(p.get_height(),2), fontsize=12, ha=\"center\", va='bottom')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DAq5zgQ1WPyV"
   },
   "source": [
    "### Cross Validation\n",
    "\n",
    "Cross validation is a technique used to test the accuracy of a model's prediction on unseen data (validation sets). This is important because it can assist in picking up issues such as over/underfitting and selection bias. We used the K-fold technique to perform cross validation on our two best perfoming models, i.e. **SVD** and **BaselineOnly**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WZBbROI4WPyV"
   },
   "source": [
    "**SVD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c5QtmfZgWPyW",
    "outputId": "6654f686-bc9d-40a7-f9f0-19a406afc68e"
   },
   "outputs": [],
   "source": [
    "svd_test = SVD(n_epochs = 40, n_factors = 200, init_std_dev = 0.05, random_state=42)\n",
    "# Run 5-fold cross-validation and print results\n",
    "a = cross_validate(svd_test, data, measures=['RMSE'], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PNKRRWEEWPyY"
   },
   "source": [
    "**BaselineOnly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JQM7wuR1WPyY",
    "outputId": "833a2c58-f851-4e38-9f20-2fae08a3960b"
   },
   "outputs": [],
   "source": [
    "bsl_options = {'method': 'sgd','n_epochs': 40}\n",
    "blo_test = BaselineOnly(bsl_options=bsl_options)\n",
    "# Run 5-fold cross-validation and print results\n",
    "b = cross_validate(blo_test, data, measures=['RMSE'], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5BiBBxvR2FyY"
   },
   "source": [
    "<a id=\"analysis\"></a>\n",
    "## 7. Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xphTRd1Xlww0"
   },
   "outputs": [],
   "source": [
    "# Load the 100k dataset\n",
    "train_subset = train[:100000]\n",
    "reader = Reader(rating_scale=(train_subset['rating'].min(), train_subset['rating'].max()))\n",
    "data = Dataset.load_from_df(train_subset[['userId', 'movieId', 'rating']], reader)\n",
    "trainset, testset = train_test_split(data, test_size=.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DDeeMPW-WPya"
   },
   "source": [
    "### Hyperparameter Tuning \n",
    "\n",
    "Hyperparameter tuning is the process by which a set of ideal hyperparameters are chosen for a model. A hyperparameter is a parameter for which the value is set manually and tuned to control the algorithm's learning process. We tested multiple parameters for our best performing model (i.e. **SVD**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cemEEPIIWPya",
    "outputId": "daae477f-bf2d-48de-d309-1e2710299712"
   },
   "outputs": [],
   "source": [
    "param_grid = {'n_epochs':[40], #[30,40,50],\n",
    "              'n_factors':[400], #[100,200,300,400],\n",
    "              'init_std_dev':[0.005], #[0.001,0.005,0.05,0.1],\n",
    "              'random_state':[42]} \n",
    "grid_SVD = GridSearchCV(SVD, cv=5, measures=['rmse'], param_grid=param_grid, n_jobs=-1)\n",
    "grid_SVD.fit(data)\n",
    "print('***Best score:***')\n",
    "print(grid_SVD.best_score['rmse'])\n",
    "print('***Best parameters:***')\n",
    "print(grid_SVD.best_params['rmse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GrhCgujuWPyd"
   },
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5HJ1If7gWPyd",
    "outputId": "63da9e41-2b73-4a90-ae59-0fa38fc4b301"
   },
   "outputs": [],
   "source": [
    "svd_test = SVD(n_epochs = 40, n_factors = 400, init_std_dev = 0.005, random_state=42)\n",
    "svd_test.fit(trainset)\n",
    "predictions = svd_test.test(testset)\n",
    "# Calculate RMSE\n",
    "svd_rmse = accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sNar9_exWPye",
    "outputId": "283b5042-37c7-4118-c757-65208a84d5a7"
   },
   "outputs": [],
   "source": [
    "# Predicted Target Values vs. Actual Target Values\n",
    "new_df = pd.DataFrame(columns=['uid', 'iid', 'rating'])\n",
    "i = 0\n",
    "for (uid, iid, rating) in testset:\n",
    "    new_df.loc[i] = [uid, iid, rating]\n",
    "    i = i+1\n",
    "true = new_df['rating']\n",
    "pred = []\n",
    "for i in predictions:\n",
    "    pred.append(i.est)\n",
    "fig,axis = plt.subplots(figsize=(8, 5))\n",
    "sns.boxplot(x=true, y=pred, palette=\"brg\")\n",
    "plt.title(\"Predicted Target Values vs. Actual Target Values\", fontsize=14)\n",
    "plt.xlabel(\"Actual Target Values\")\n",
    "plt.ylabel(\"Predicted Target Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DWi0DhWiL9ew"
   },
   "outputs": [],
   "source": [
    "params = {'n_epochs':40, #[30,40,50],\n",
    "          'n_factors':400, #[100,200,300],\n",
    "          'init_std_dev':0.005, #[0.005,0.05,0.1],\n",
    "          'random_state':[42]} \n",
    "metrics = {\"RMSE\": np.sqrt(mean_squared_error(true, pred))}\n",
    "\n",
    "experiment.log_parameters(params)\n",
    "experiment.log_metrics(metrics)\n",
    "experiment.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VVzGHxoQL9es"
   },
   "source": [
    "<a id=\"conclusion\"></a>\n",
    "## 8. Conclusion\n",
    "In this project, we succeeded in building an unsupervised machine learning model that is able to recommend movies based on content-based or collaborative filtering and is capable of accurately predicting how a user will rate a movie they have not yet viewed, based on their historical preferences. Our top performing model has a root mean squared error (RMSE) of 0.78, based on a testing set submitted to the EDSA [Kaggle](https://www.kaggle.com/c/edsa-recommender-system-predict/leaderboard) competition.  \n",
    "\n",
    "\n",
    "The singular value decomposition (SVD) algorithm is a baseline approach to recommender systems, as it has a broad range of applications including dimensionality reduction, solving linear inverse problems, and data fitting. The SVD algorithm generally performs better on large datasets compared to some other models as it decomposes a matrix into constituent arrays of feature vectors corresponding to each row and each column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j4eHDTwoL9et"
   },
   "source": [
    "<a id=\"save\"></a>\n",
    "## 9. Save Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LjAQTPZmL9et"
   },
   "outputs": [],
   "source": [
    "# Train model on whole dataset\n",
    "reader = Reader(rating_scale=(train['rating'].min(), train['rating'].max()))\n",
    "data = Dataset.load_from_df(train[['userId', 'movieId', 'rating']], reader)\n",
    "trainset = data.build_full_trainset()\n",
    "svd = SVD(n_epochs = 40, n_factors = 400, init_std_dev = 0.005, random_state=42, verbose=True)\n",
    "svd.fit(trainset)\n",
    "\n",
    "# Create Kaggle submission file\n",
    "predictions = []\n",
    "for i, row in test.iterrows():\n",
    "    x = (svd.predict(row.userId, row.movieId))\n",
    "    pred = x[3]\n",
    "    predictions.append(pred)\n",
    "test['Id'] = test['userId'].map(str) +'_'+ test['movieId'].map(str)\n",
    "results = pd.DataFrame({\"Id\":test['Id'],\"rating\": predictions})\n",
    "results.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3VAGBYfeWPyk"
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "model_save_path = \"SVD.pkl\"\n",
    "with open(model_save_path,'wb') as file:\n",
    "    pickle.dump(svd,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7kwqZ3ToL9ey"
   },
   "outputs": [],
   "source": [
    "experiment.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6S0qj23KvTil"
   },
   "source": [
    "<a id=\"ref\"></a>\n",
    "## 10. References\n",
    "<a id=\"ref1\"></a>[1] Ansari, A. (n.d.). Internet Recommendation Systems. Retrieved August/September, 2000, from https://www0.gsb.columbia.edu/mygsb/faculty/research/pubfiles/385/Internet Recommendation Systems.pdf\n",
    "\n",
    "<a id=\"ref2\"></a>[2] Shi, C. (2017, June 27). A Hybrid Recommender with Yelp Challenge. Retrieved from https://nycdatascience.com/blog/student-works/yelp-recommender-part-1/\n",
    "\n",
    "<a id=\"ref3\"></a>[3] Wikipedia: t-distributed stochastic neighbor embedding. Retrieved from https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding\n",
    "\n",
    "<a id=\"ref4\"></a>[4] Jolliffe, I.T. & Cadima, J. (2016, April 13). Principal component analysis: a review and recent developments. Retrieved from https://royalsocietypublishing.org/doi/10.1098/rsta.2015.0202\n",
    "\n",
    "<a id=\"ref5\"></a>[5] Ajitsaria, A. (n.d.). Build a Recommendation Engine With Collaborative Filtering. Retrieved from https://realpython.com/build-recommendation-engine-collaborative-filtering\n",
    "\n",
    "<a id=\"ref6\"></a>[6] Funk, S. (2006, Deccember 11). Netflix Update: Try This at Home. Retrieved from https://sifter.org/simon/journal/20061211.html\n",
    "\n",
    "<a id=\"ref7\"></a>[7] Recommender Systems: Content-based Filtering. Retrieved from http://recommender-systems.org/content-based-filtering/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "320c1f05b41b6296d6cdeadbc8f37198b22e160db062b16d8b8cc9d95c25d782"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
